{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from afinn import Afinn\n",
    "\n",
    "np.set_printoptions(precision=2,linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"movie_reviews.csv\")\n",
    "\n",
    "reviews = np.array(dataset[\"review\"])\n",
    "sentiments = np.array(dataset[\"sentiment\"])\n",
    "\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_reviews_ids = [726, 3533, 13010]\n",
    "\n",
    "#normalize dataset\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Very simply, they are all the syndicated episodes and NOT the original uncut/unedited NBC episodes. It is NOT the complete first season, all eps are edited to conform to 21:00 for syndication meaning jokes are cut, an extra commercial fade is included, all of the Harvey Korman intros are not here...very poorly done! Shame on a series I've been waiting for....booooooooooooooooo! If you're a true die hard Mama fan, don't buy this and go to http://www2.warnerbros.com/web/main/help/whv/customer_service.jsp and send them comments on why we're unhappy on this butcher job to a classic sitcom!\n",
      "Actual Sentiment negative\n",
      "Predicted Sentiment Polarity -2.0\n",
      "------------------------------------------------------------\n",
      "Review: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment positive\n",
      "Predicted Sentiment Polarity 3.0\n",
      "------------------------------------------------------------\n",
      "Review: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment positive\n",
      "Predicted Sentiment Polarity -3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Afinn Lexicon\n",
    "afn = Afinn(emoticons=True)\n",
    "\n",
    "for review, sentiment in zip(test_reviews[sample_reviews_ids], test_sentiments[sample_reviews_ids]):\n",
    "    print(\"Review:\", review)\n",
    "    print(\"Actual Sentiment\", sentiment)\n",
    "    print(\"Predicted Sentiment Polarity\", afn.score(review))\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = [\"positive\" if score >=1.0 else \"negative\" for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7118\n",
      "Precision: 0.7289\n",
      "Recall: 0.7118\n",
      "F1 Score: 0.7062\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.85      0.75      7510\n",
      "    negative       0.79      0.57      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.73      0.71      0.71     15000\n",
      "weighted avg       0.73      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-71f2127ae5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                        \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                        classes=[\"positive\", \"negative\"])\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m def plot_model_decision_surface(clf, train_features, train_labels,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                 \u001b[0mplot_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRdYlBu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                 markers=None, alphas=None, colors=None):\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                   labels=classes)\n\u001b[1;32m     52\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 53\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     54\u001b[0m                                                   codes=level_labels), \n\u001b[1;32m     55\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "                                       predicted_labels=predicted_sentiments,\n",
    "                                       classes=[\"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWordNet Lexicon\n",
    "awesome = list(swn.sent_synsets(\"awesome\", \"a\"))[0]\n",
    "print(\"Positive Polarity Score:\", awesome.pos_score())\n",
    "print(\"Negative Polarity Score\", awesome.neg_score())\n",
    "print(\"Objective Score:\", awesome.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review,\n",
    "                                           verbose=False):\n",
    "\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
    "                                         norm_neg_score, norm_final_score]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                             ['Predicted Sentiment', 'Objectivity',\n",
    "                                                              'Positive', 'Negative', 'Overall']], \n",
    "                                                             codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Very simply, they are all the syndicated episodes and NOT the original uncut/unedited NBC episodes. It is NOT the complete first season, all eps are edited to conform to 21:00 for syndication meaning jokes are cut, an extra commercial fade is included, all of the Harvey Korman intros are not here...very poorly done! Shame on a series I've been waiting for....booooooooooooooooo! If you're a true die hard Mama fan, don't buy this and go to http://www2.warnerbros.com/web/main/help/whv/customer_service.jsp and send them comments on why we're unhappy on this butcher job to a classic sitcom!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            negative        0.77     0.09     0.14   -0.04\n",
      "------------------------------------------------------------\n",
      "Review: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.74      0.2     0.06    0.14\n",
      "------------------------------------------------------------\n",
      "Review: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.81     0.12     0.07    0.05\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_reviews_ids], test_sentiments[sample_reviews_ids]):\n",
    "    print(\"Review:\", review)\n",
    "    print(\"Actual Sentiment:\", sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6843\n",
      "Precision: 0.687\n",
      "Recall: 0.6843\n",
      "F1 Score: 0.6831\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.66      0.75      0.70      7510\n",
      "    negative       0.71      0.62      0.66      7490\n",
      "\n",
      "    accuracy                           0.68     15000\n",
      "   macro avg       0.69      0.68      0.68     15000\n",
      "weighted avg       0.69      0.68      0.68     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-7c84598cef06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredicted_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                       \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       classes=[\"positive\", \"negative\"])\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m def plot_model_decision_surface(clf, train_features, train_labels,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                 \u001b[0mplot_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRdYlBu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                 markers=None, alphas=None, colors=None):\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                   labels=classes)\n\u001b[1;32m     52\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 53\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     54\u001b[0m                                                   codes=level_labels), \n\u001b[1;32m     55\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "                                      predicted_labels=predicted_sentiments,\n",
    "                                      classes=[\"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER Lexicon\n",
    "def analyze_sentiment_vader_lexicon(review, threshold=.1,verbose=False):\n",
    "    \n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores[\"compound\"]\n",
    "    final_sentiment = \"positive\" if agg_score >= threshold else \"negative\"\n",
    "    \n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores[\"pos\"],2)*100)+\"%\"\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores[\"neg\"], 2)*100)+\"%\"\n",
    "        neutral = str(round(scores[\"neu\"], 2)*100)+\"%\"\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, negative, neutral]], \n",
    "                                       columns=pd.MultiIndex(\n",
    "                                           levels=[[\"SENTIMENT STATS:\"],\n",
    "                                                   [\"Predicted Sentiment\", \"Polarity Score\", \"Positive\", \"Negative\", \"Neutral\"]\n",
    "                                                  ], codes=[[0,0,0,0,0], [0,1,2,3,4]])\n",
    "                                      )\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: Very simply, they are all the syndicated episodes and NOT the original uncut/unedited NBC episodes. It is NOT the complete first season, all eps are edited to conform to 21:00 for syndication meaning jokes are cut, an extra commercial fade is included, all of the Harvey Korman intros are not here...very poorly done! Shame on a series I've been waiting for....booooooooooooooooo! If you're a true die hard Mama fan, don't buy this and go to http://www2.warnerbros.com/web/main/help/whv/customer_service.jsp and send them comments on why we're unhappy on this butcher job to a classic sitcom!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                                    \n",
      "  Predicted Sentiment Polarity Score            Positive Negative Neutral\n",
      "0            negative          -0.84  7.000000000000001%    15.0%   78.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                                     \n",
      "  Predicted Sentiment Polarity Score Positive             Negative Neutral\n",
      "0            negative          -0.16    16.0%  14.000000000000002%   69.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            positive           0.49    11.0%    11.0%   77.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_reviews_ids], test_sentiments[sample_reviews_ids]):\n",
    "    print(\"REVIEW:\", review)\n",
    "    print(\"Actual Sentiment:\", sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=.4, verbose=True)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7108\n",
      "Precision: 0.7237\n",
      "Recall: 0.7108\n",
      "F1 Score: 0.7065\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.83      0.74      7510\n",
      "    negative       0.78      0.59      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.72      0.71      0.71     15000\n",
      "weighted avg       0.72      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-def02168bb9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredicted_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_vader_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                       \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       classes=[\"positive\", \"negative\"])\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m def plot_model_decision_surface(clf, train_features, train_labels,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                 \u001b[0mplot_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRdYlBu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                 markers=None, alphas=None, colors=None):\n",
      "\u001b[0;32m/mnt/c/Users/ptara/OneDrive/Documentos/python/Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                   labels=classes)\n\u001b[1;32m     52\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 53\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     54\u001b[0m                                                   codes=level_labels), \n\u001b[1;32m     55\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=.4) for review in test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments,\n",
    "                                      predicted_labels=predicted_sentiments,\n",
    "                                      classes=[\"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
